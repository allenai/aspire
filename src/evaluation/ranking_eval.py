"""
Evaluate the rankings generated by sentence similarity models.
"""
import sys
import os
import errno
import argparse
import statistics
import codecs
import json
import csv
import comet_ml as cml
from scipy import stats as scipystats

import rank_metrics as rm


facet2folds = {
    "background": {"fold1_dev": ["3264891_background", "1936997_background", "11844559_background",
                                 "52194540_background", "1791179_background", "6431039_background",
                                 "6173686_background", "7898033_background"],
                   "fold2_dev": ["5764728_background", "10014168_background", "10695055_background",
                                 "929877_background", "1587_background", "51977123_background",
                                 "8781666_background", "189897839_background"],
                   "fold1_test": ["5764728_background", "10014168_background", "10695055_background",
                                  "929877_background", "1587_background", "51977123_background",
                                  "8781666_background", "189897839_background"],
                   "fold2_test": ["3264891_background", "1936997_background", "11844559_background",
                                  "52194540_background", "1791179_background", "6431039_background",
                                  "6173686_background", "7898033_background"]},
    "method": {"fold1_dev": ["189897839_method", "1791179_method", "11310392_method", "2468783_method",
                             "13949438_method", "5270848_method", "52194540_method", "929877_method"],
               "fold2_dev": ["5052952_method", "10010426_method", "102353905_method", "174799296_method",
                             "1198964_method", "53080736_method", "1936997_method", "80628431_method",
                             "53082542_method"],
               "fold1_test": ["5052952_method", "10010426_method", "102353905_method", "174799296_method",
                              "1198964_method", "53080736_method", "1936997_method", "80628431_method",
                              "53082542_method"],
               "fold2_test": ["189897839_method", "1791179_method", "11310392_method", "2468783_method",
                              "13949438_method", "5270848_method", "52194540_method", "929877_method"]},
    "result": {"fold1_dev": ["2090262_result", "174799296_result", "11844559_result", "2468783_result",
                             "1306065_result", "5052952_result", "3264891_result", "8781666_result"],
               "fold2_dev": ["2865563_result", "10052042_result", "11629674_result", "1587_result",
                             "1198964_result", "53080736_result", "2360770_result", "80628431_result",
                             "6431039_result"],
               "fold1_test": ["2865563_result", "10052042_result", "11629674_result", "1587_result",
                              "1198964_result", "53080736_result", "2360770_result", "80628431_result",
                              "6431039_result"],
               "fold2_test": ["2090262_result", "174799296_result", "11844559_result", "2468783_result",
                              "1306065_result", "5052952_result", "3264891_result", "8781666_result"]},
    "all": {"fold1_dev": ["3264891_background", "1936997_background", "11844559_background",
                          "52194540_background", "1791179_background", "6431039_background",
                          "6173686_background", "7898033_background", "189897839_method",
                          "1791179_method", "11310392_method", "2468783_method", "13949438_method",
                          "5270848_method", "52194540_method", "929877_method", "2090262_result",
                          "174799296_result", "11844559_result", "2468783_result", "1306065_result",
                          "5052952_result", "3264891_result", "8781666_result"],
            "fold2_dev": ["5764728_background", "10014168_background", "10695055_background",
                          "929877_background", "1587_background", "51977123_background",
                          "8781666_background", "189897839_background", "5052952_method", "10010426_method",
                          "102353905_method", "174799296_method", "1198964_method", "53080736_method",
                          "1936997_method", "80628431_method", "53082542_method", "2865563_result",
                          "10052042_result", "11629674_result", "1587_result", "1198964_result",
                          "53080736_result", "2360770_result", "80628431_result", "6431039_result"],
            "fold1_test": ["5764728_background", "10014168_background", "10695055_background",
                           "929877_background", "1587_background", "51977123_background", "8781666_background",
                           "189897839_background", "5052952_method", "10010426_method", "102353905_method",
                           "174799296_method", "1198964_method", "53080736_method", "1936997_method",
                           "80628431_method", "53082542_method", "2865563_result", "10052042_result",
                           "11629674_result", "1587_result", "1198964_result", "53080736_result",
                           "2360770_result", "80628431_result", "6431039_result"],
            "fold2_test": ["3264891_background", "1936997_background", "11844559_background",
                           "52194540_background", "1791179_background", "6431039_background",
                           "6173686_background", "7898033_background", "189897839_method", "1791179_method",
                           "11310392_method", "2468783_method", "13949438_method", "5270848_method",
                           "52194540_method", "929877_method", "2090262_result", "174799296_result",
                           "11844559_result", "2468783_result", "1306065_result", "5052952_result",
                           "3264891_result", "8781666_result"]
            }
}


def create_dir(dir_name):
    """
    Create the directory whose name is passed.
    :param dir_name: String saying the name of directory to create.
    :return: None.
    """
    # Create output directory if it doesnt exist.
    try:
        os.makedirs(dir_name)
        print('Created: {}.'.format(dir_name))
    except OSError as ose:
        # For the case of *file* by name of out_dir existing
        if (not os.path.isdir(dir_name)) and (ose.errno == errno.EEXIST):
            sys.stderr.write('IO ERROR: Could not create output directory\n')
            sys.exit(1)
        # If its something else you don't know; report it and exit.
        if ose.errno != errno.EEXIST:
            sys.stderr.write('OS ERROR: {:d}: {:s}: {:s}\n'.format(ose.errno,
                                                                   ose.strerror,
                                                                   dir_name))
            sys.exit(1)


def recall_at_k(ranked_rel, atk, max_total_relevant):
    """
    Compute recall at k.
    :param ranked_rel: list(int); ranked list of relevance judged data.
    :param atk: int; rank at which to compute metric.
    :param max_total_relevant: int; maximum relevant to consider in
        case there are more relevant in total.
    :return: recall: float.
    """
    total_relevant = sum(ranked_rel)
    total_relevant = min(max_total_relevant, total_relevant)
    relatk = sum(ranked_rel[:atk])
    if total_relevant > 0:
        recall_atk = float(relatk)/total_relevant
    else:
        recall_atk = 0.0
    return recall_atk
    

def compute_metrics(ranked_judgements, pr_atks, threshold_grade):
    """
    Given the ranked judgements compute precision, recall and f1.
    :param ranked_judgements: list(int); graded or binary relevances in rank order.
    :param pr_atks: list(int); the @K values to use for computing precision and recall.
    :param threshold_grade: int; Assuming 0-3 graded relevances, threshold at some point
        and convert graded to binary relevance. If passed also compute NDCG.
    :return:
    """
    metrics = {}
    graded_judgements = ranked_judgements
    ranked_judgements = [1 if rel >= threshold_grade else 0 for rel in graded_judgements]
    # Use the full set of candidate not the pr_atk.
    ndcg = rm.ndcg_at_k(graded_judgements, len(ranked_judgements))
    ndcg_20 = rm.ndcg_at_k(graded_judgements, 20)
    ndcg_50 = rm.ndcg_at_k(graded_judgements, 50)
    for atk in [5, 10, 15, 20, 25]:
        ndcg_pr_atk = rm.ndcg_at_k(graded_judgements, int((atk/100)*len(ranked_judgements)))
        metrics[f'ndcg%{atk}'] = float(ndcg_pr_atk)
    max_total_relevant = sum(ranked_judgements)
    for atk in pr_atks:
        recall = recall_at_k(ranked_rel=ranked_judgements,
                             atk=atk, max_total_relevant=max_total_relevant)
        precision = rm.precision_at_k(r=ranked_judgements, k=atk)
        f1 = 2*precision*recall/(precision + recall) if (precision + recall) > 0 else 0.0
        metrics[f'precision@{atk}'] = float(precision)
        metrics[f'recall@{atk}'] = float(recall)
        metrics[f'f1@{atk}'] = float(f1)
    r_precision = rm.r_precision(r=ranked_judgements)
    av_precision = rm.average_precision(r=ranked_judgements)
    reciprocal_rank = rm.mean_reciprocal_rank(rs=[ranked_judgements])
    metrics['r_precision'] = float(r_precision)
    metrics['av_precision'] = float(av_precision)
    metrics['reciprocal_rank'] = float(reciprocal_rank)
    metrics['ndcg'] = float(ndcg)
    metrics['ndcg@20'] = float(ndcg_20)
    metrics['ndcg@50'] = float(ndcg_50)
    return metrics


def aggregate_metrics(query_metrics):
    """
    Given metrics over individual queries aggregate over different
    queries. Simple average for now.
    :param query_metrics: dict(query_id: metrics_dict from compute_metrics)
    :return:
    """
    precision5, precision10, precision20, recall20, f120 = 0.0, 0.0, 0.0, 0.0, 0.0
    av_precision, mrr, ndcg, r_precision = 0.0, 0.0, 0.0, 0.0
    ndcg_20, ndcg_50 = 0.0, 0.0
    ndcg_pr_5, ndcg_pr_10, ndcg_pr_15, ndcg_pr_20, ndcg_pr_25 = 0.0, 0.0, 0.0, 0.0, 0.0
    for queri_id, metrics in query_metrics.items():
        precision5 += metrics['precision@5']
        precision10 += metrics['precision@10']
        precision20 += metrics['precision@20']
        recall20 += metrics['recall@20']
        f120 += metrics['f1@20']
        av_precision += metrics['av_precision']
        mrr += metrics['reciprocal_rank']
        r_precision += metrics['r_precision']
        if 'ndcg' in metrics:
            ndcg += metrics['ndcg']
            ndcg_20 += metrics['ndcg@20']
            ndcg_50 += metrics['ndcg@50']
            ndcg_pr_5 += metrics['ndcg%5']
            ndcg_pr_10 += metrics['ndcg%10']
            ndcg_pr_15 += metrics['ndcg%15']
            ndcg_pr_20 += metrics['ndcg%20']
            ndcg_pr_25 += metrics['ndcg%25']
    num_queries = len(query_metrics)
    aggmetrics = {
        'precision@5': precision5/num_queries,
        'precision@10': precision10/num_queries,
        'precision@20': precision20/num_queries,
        'recall@20': recall20/num_queries,
        'f1@20': f120/num_queries,
        'r_precision': r_precision/num_queries,
        'mean_av_precision': av_precision/num_queries,
        'mean_reciprocal_rank': mrr/num_queries,
        'ndcg': ndcg/num_queries,
        'ndcg@20': ndcg_20/num_queries,
        'ndcg@50': ndcg_50/num_queries,
        'ndcg%5': ndcg_pr_5/num_queries,
        'ndcg%10': ndcg_pr_10/num_queries,
        'ndcg%15': ndcg_pr_15/num_queries,
        'ndcg%20': ndcg_pr_20/num_queries,
        'ndcg%25': ndcg_pr_25/num_queries
    }
    return aggmetrics


def aggregate_metrics_crossval(query_metrics, split_str, facet_str):
    """
    Given metrics over individual queries aggregate over different
    queries. Simple average for now.
    :param query_metrics: dict(query_id: metrics_dict from compute_metrics)
    :param split_str: string; {dev, test}
    :param facet_str: string; {background, method, result}
    :param query_metadata: dict(qpid_facet: dict(metadata))
    :return:
    """
    aggmetrics = {
        'precision@5': [],
        'precision@10': [],
        'precision@20': [],
        'recall@20': [],
        'f1@20': [],
        'r_precision': [],
        'mean_av_precision': [],
        'mean_reciprocal_rank': [],
        'ndcg': [],
        'ndcg@20': [],
        'ndcg@50': [],
        'ndcg%5': [],
        'ndcg%10': [],
        'ndcg%15': [],
        'ndcg%20': [],
        'ndcg%25': []
    }
    # For dev only use a part of the fold - using both makes it identical to test.
    if split_str == 'dev':
        folds = ['fold1_{:s}'.format(split_str)]
    elif split_str == 'test':
        folds = ['fold1_{:s}'.format(split_str), 'fold2_{:s}'.format(split_str)]
    for fold_str in folds:
        fold_pids = facet2folds[facet_str][fold_str]
        precision5, precision10, precision20, recall20, f120 = 0.0, 0.0, 0.0, 0.0, 0.0
        ndcg_20, ndcg_50 = 0.0, 0.0
        ndcg_pr_5, ndcg_pr_10, ndcg_pr_15, ndcg_pr_20, ndcg_pr_25 = 0.0, 0.0, 0.0, 0.0, 0.0
        av_precision, mrr, ndcg, r_precision = 0.0, 0.0, 0.0, 0.0
        for query_id in fold_pids:
            # Aggregate across paper types in the fold.
            metrics = query_metrics[query_id]
            # Aggregate across all papers in the fold
            precision5 += metrics['precision@5']
            precision10 += metrics['precision@10']
            precision20 += metrics['precision@20']
            recall20 += metrics['recall@20']
            f120 += metrics['f1@20']
            av_precision += metrics['av_precision']
            mrr += metrics['reciprocal_rank']
            r_precision += metrics['r_precision']
            ndcg += metrics['ndcg']
            ndcg_20 += metrics['ndcg@20']
            ndcg_50 += metrics['ndcg@50']
            ndcg_pr_5 += metrics['ndcg%5']
            ndcg_pr_10 += metrics['ndcg%10']
            ndcg_pr_15 += metrics['ndcg%15']
            ndcg_pr_20 += metrics['ndcg%20']
            ndcg_pr_25 += metrics['ndcg%25']
        # Average all folds
        num_queries = len(fold_pids)
        precision5, precision10, precision20 = precision5/num_queries, precision10/num_queries, \
                                               precision20/num_queries
        recall20, f120 = recall20/num_queries, f120/num_queries
        av_precision = av_precision/num_queries
        mrr, ndcg, r_precision = mrr/num_queries, ndcg/num_queries, r_precision/num_queries
        ndcg_20, ndcg_50 = ndcg_20/num_queries, ndcg_50/num_queries
        ndcg_pr_5, ndcg_pr_10, ndcg_pr_15, ndcg_pr_20, ndcg_pr_25 = ndcg_pr_5/num_queries, ndcg_pr_10/num_queries,\
                                                                    ndcg_pr_15/num_queries, ndcg_pr_20/num_queries, \
                                                                    ndcg_pr_25/num_queries
        # Save the averaged metric for every fold.
        aggmetrics['precision@5'].append(precision5)
        aggmetrics['precision@10'].append(precision10)
        aggmetrics['precision@20'].append(precision20)
        aggmetrics['recall@20'].append(recall20)
        aggmetrics['f1@20'].append(f120)
        aggmetrics['r_precision'].append(r_precision)
        aggmetrics['mean_av_precision'].append(av_precision)
        aggmetrics['mean_reciprocal_rank'].append(mrr)
        aggmetrics['ndcg'].append(ndcg)
        aggmetrics['ndcg@20'].append(ndcg_20)
        aggmetrics['ndcg@50'].append(ndcg_50)
        aggmetrics['ndcg%5'].append(ndcg_pr_5)
        aggmetrics['ndcg%10'].append(ndcg_pr_10)
        aggmetrics['ndcg%15'].append(ndcg_pr_15)
        aggmetrics['ndcg%20'].append(ndcg_pr_20)
        aggmetrics['ndcg%25'].append(ndcg_pr_25)

    aggmetrics = {
        'precision@5': statistics.mean(aggmetrics['precision@5']),
        'precision@10': statistics.mean(aggmetrics['precision@10']),
        'precision@20': statistics.mean(aggmetrics['precision@20']),
        'recall@20': statistics.mean(aggmetrics['recall@20']),
        'f1@20': statistics.mean(aggmetrics['f1@20']),
        'r_precision': statistics.mean(aggmetrics['r_precision']),
        'mean_av_precision': statistics.mean(aggmetrics['mean_av_precision']),
        'mean_reciprocal_rank': statistics.mean(aggmetrics['mean_reciprocal_rank']),
        'ndcg': statistics.mean(aggmetrics['ndcg']),
        'ndcg@20': statistics.mean(aggmetrics['ndcg@20']),
        'ndcg@50': statistics.mean(aggmetrics['ndcg@50']),
        'ndcg%5': statistics.mean(aggmetrics['ndcg%5']),
        'ndcg%10': statistics.mean(aggmetrics['ndcg%10']),
        'ndcg%15': statistics.mean(aggmetrics['ndcg%15']),
        'ndcg%20': statistics.mean(aggmetrics['ndcg%20']),
        'ndcg%25': statistics.mean(aggmetrics['ndcg%25']),
    }
    return aggmetrics
    

def read_unf_relevances(data_path, run_path, dataset, method):
    """
    Read the gold data and the model rankings and the relevances for the
    model.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :return: qpid2rankedcand_relevances: dict('qpid_facet': [relevances]);
        candidate gold relevances for the candidates in order ranked by the
        model.
    """
    gold_fname = os.path.join(data_path, 'test-pid2anns-{:s}.json'.format(dataset))
    ranked_fname = os.path.join(run_path, 'test-pid2pool-{:s}-{:s}-ranked.json'.format(dataset, method))
    # Load gold test data (citations).
    with codecs.open(gold_fname, 'r', 'utf-8') as fp:
        pid2pool_source = json.load(fp)
        num_query = len(pid2pool_source)
        print('Gold query pids: {:d}'.format(num_query))
        pid2rels_gold = {}
        for qpid, pool_rel in pid2pool_source.items():
            pool = pool_rel['cands']
            cands_rels = pool_rel['relevance_adju']
            pid2rels_gold['{:s}'.format(qpid)] = \
                dict([(pid, rel) for pid, rel in zip(pool, cands_rels)])
    # Load ranked predictions on test data with methods.
    with codecs.open(ranked_fname, 'r', 'utf-8') as fp:
        pid2ranks = json.load(fp)
        print('Valid ranked query pids: {:d}'.format(len(pid2ranks)))
        qpid2rankedcand_relevances = {}
        for qpid, citranks in pid2ranks.items():
            candpids = [pid_score[0] for pid_score in citranks]
            cand_relevances = [pid2rels_gold[qpid][pid] for pid in candpids]
            qpid2rankedcand_relevances[qpid] = cand_relevances
    return qpid2rankedcand_relevances


def read_facet_specific_relevances(data_path, run_path, dataset, facet, method):
    """
    Read the gold data and the model rankings and the relevances for the
    model.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :param facet: string; facet for eval.
    :return: qpid2rankedcand_relevances: dict('qpid_facet': [relevances]);
        candidate gold relevances for the candidates in order ranked by the
        model.
    """
    gold_fname = os.path.join(data_path, 'test-pid2anns-{:s}-{:s}.json'.format(dataset, facet))
    ranked_fname = os.path.join(run_path, 'test-pid2pool-{:s}-{:s}-{:s}-ranked.json'.format(dataset, method, facet))
    # Load gold test data (citations).
    with codecs.open(gold_fname, 'r', 'utf-8') as fp:
        pid2pool_source = json.load(fp)
        num_query = len(pid2pool_source)
        print('Gold query pids: {:d}'.format(num_query))
        pid2rels_gold = {}
        for qpid, pool_rel in pid2pool_source.items():
            pool = pool_rel['cands']
            cands_rels = pool_rel['relevance_adju']
            pid2rels_gold['{:s}_{:s}'.format(qpid, facet)] = \
                dict([(pid, rel) for pid, rel in zip(pool, cands_rels)])
    # Load ranked predictions on test data with methods.
    with codecs.open(ranked_fname, 'r', 'utf-8') as fp:
        pid2ranks = json.load(fp)
        print('Valid ranked query pids: {:d}'.format(len(pid2ranks)))
        qpid2rankedcand_relevances = {}
        for qpid, citranks in pid2ranks.items():
            candpids = [pid_score[0] for pid_score in citranks]
            cand_relevances = [pid2rels_gold['{:s}_{:s}'.format(qpid, facet)][pid] for pid in candpids]
            qpid2rankedcand_relevances['{:s}_{:s}'.format(qpid, facet)] = cand_relevances
    return qpid2rankedcand_relevances


def read_all_facet_relevances(data_path, run_path, dataset, method, facets):
    """
    Read the gold data and the model rankings and the relevances for the
    model.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :param facets: list(string); what facets to read/what counts as "all".
    :return: qpid2rankedcand_relevances: dict('qpid_facet': [relevances]);
        candidate gold relevances for the candidates in order ranked by the
        model.
    """
    qpid2rankedcand_relevances = {}
    for facet in facets:
        print('Reading facet: {:s}'.format(facet))
        gold_fname = os.path.join(data_path, 'test-pid2anns-{:s}-{:s}.json'.format(dataset, facet))
        ranked_fname = os.path.join(run_path, 'test-pid2pool-{:s}-{:s}-{:s}-ranked.json'.format(dataset, method, facet))
        # Load gold test data (citations).
        with codecs.open(gold_fname, 'r', 'utf-8') as fp:
            pid2pool_source = json.load(fp)
            num_query = len(pid2pool_source)
            print('Gold query pids: {:d}'.format(num_query))
            pid2rels_gold = {}
            for qpid, pool_rel in pid2pool_source.items():
                pool = pool_rel['cands']
                cands_rels = pool_rel['relevance_adju']
                pid2rels_gold['{:s}_{:s}'.format(qpid, facet)] = \
                    dict([(pid, rel) for pid, rel in zip(pool, cands_rels)])
        # Load ranked predictions on test data with methods.
        with codecs.open(ranked_fname, 'r', 'utf-8') as fp:
            pid2ranks = json.load(fp)
            print('Valid ranked query pids: {:d}'.format(len(pid2ranks)))
            for qpid, citranks in pid2ranks.items():
                candpids = [pid_score[0] for pid_score in citranks]
                cand_relevances = [pid2rels_gold['{:s}_{:s}'.format(qpid, facet)][pid] for pid in candpids]
                qpid2rankedcand_relevances['{:s}_{:s}'.format(qpid, facet)] = cand_relevances
    print('Total queries: {:d}'.format(len(qpid2rankedcand_relevances)))
    return qpid2rankedcand_relevances


def graded_eval_pool_rerank_faceted(data_path, run_path, method, dataset, facet, split_str, comet_exp_key):
    """
    Evaluate the re-ranked pool for the faceted data. Anns use graded relevance scores.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :param facet: string; facet for eval.
    :param split_str: {dev, test}
    :param comet_exp_key: string; to allow reporting metrics to existing experiments.
    :return:
    """
    print(f'EVAL SPLIT: {split_str}')
    ATKS = [5, 10, 20]
    with codecs.open(os.path.join(data_path, f'{dataset}-queries-release.csv')) as csvfile:
        reader = csv.DictReader(csvfile)
        query_metadata = dict([('{:s}_{:s}'.format(row['pid'], row['facet']), row) for row in reader])

    perq_out_fname = os.path.join(run_path, 'test-pid2pool-{:s}-{:s}-{:s}-perq.csv'.format(dataset, method, facet))
    if facet == 'all':
        qpid2rankedcand_relevances = read_all_facet_relevances(data_path=data_path, run_path=run_path,
                                                               dataset=dataset, method=method,
                                                               facets=['background', 'method', 'result'])
    else:
        qpid2rankedcand_relevances = read_facet_specific_relevances(data_path=data_path, run_path=run_path,
                                                                    dataset=dataset, facet=facet, method=method)
    # Go over test papers and compute metrics.
    all_metrics = {}
    num_cands = 0.0
    num_queries = 0.0
    perq_file = codecs.open(perq_out_fname, 'w', 'utf-8')
    perq_csv = csv.DictWriter(perq_file, extrasaction='ignore',
                              fieldnames=['paper_id', 'title',
                                          'recall@5', 'precision@5', 'f1@5',
                                          'recall@10', 'precision@10', 'f1@10',
                                          'recall@20', 'precision@20', 'f1@20',
                                          'r_precision', 'av_precision', 'reciprocal_rank', 'ndcg',
                                          'ndcg@20', 'ndcg%20', 'paper type'])
    perq_csv.writeheader()
    print('Precision and recall at rank: {:}'.format(ATKS))
    for qpid_facet, qranked_judgements in qpid2rankedcand_relevances.items():
        all_metrics[qpid_facet] = compute_metrics(qranked_judgements, pr_atks=ATKS,
                                                  threshold_grade=2)
        num_cands += len(qranked_judgements)
        num_queries += 1
        metrics = all_metrics[qpid_facet]
        metrics['paper_id'] = qpid_facet
        metrics['title'] = query_metadata[qpid_facet]['title']
        perq_csv.writerow(metrics)
    aggmetrics = aggregate_metrics_crossval(query_metrics=all_metrics,
                                            facet_str=facet, split_str=split_str)
    print('Wrote: {:s}'.format(perq_file.name))
    perq_file.close()
    print('Total queries: {:d}; Total candidates: {:d}'.format(int(num_queries), int(num_cands)))
    print('R-Precision; Precision@5; Precision@10; Precision@20; Recall@20; MAPrecision; NDCG; NDCG@20; NDCG%20')
    print('{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\n'.
          format(aggmetrics['r_precision'],
                 aggmetrics['precision@5'], aggmetrics['precision@10'], aggmetrics['precision@20'],
                 aggmetrics['recall@20'], aggmetrics['mean_av_precision'], aggmetrics['ndcg'],
                 aggmetrics['ndcg@20'], aggmetrics['ndcg%20']))
    # print('NDCG; NDCG%5; NDCG%10; NDCG%15; NDCG%20; NDCG%25')
    # print('{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\n'.
    #       format(aggmetrics['ndcg'], aggmetrics['ndcg%5'], aggmetrics['ndcg%10'], aggmetrics['ndcg%15'],
    #              aggmetrics['ndcg%20'], aggmetrics['ndcg%25']))
    
    # Log metrics to comet.ml.
    if comet_exp_key:
        run_name = os.path.basename(run_path)
        cml_experiment = cml.ExistingExperiment(previous_experiment=comet_exp_key, display_summary_level=0)
        cml_experiment.set_name(run_name)
        display_metrics = {'r_precision': aggmetrics['r_precision'],
                           'precision@5': aggmetrics['precision@5'],
                           'precision@10': aggmetrics['precision@10'],
                           'precision@20': aggmetrics['precision@20'],
                           'recall@20': aggmetrics['recall@20'],
                           'ndcg': aggmetrics['ndcg'],
                           'ndcg@20': aggmetrics['ndcg@20'],
                           'ndcg%20': aggmetrics['ndcg%20'],
                           'mean_av_precision': aggmetrics['mean_av_precision']}
        cml_experiment.log_metrics(display_metrics, prefix=f'{dataset}-{split_str}-{facet}')
        cml_experiment.log_table(perq_out_fname, prefix=f'{dataset}-{split_str}-{facet}')


def graded_eval_pool_rerank_unf(data_path, run_path, method, dataset, split, comet_exp_key):
    """
    Evaluate the re-ranked pool for unfaceted data. Anns use graded relevance scores.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :param split: string; {dev, test}
    :return:
    """
    ATKS = [5, 10, 20]
    with codecs.open(os.path.join(data_path, f'{dataset}-evaluation_splits.json'), 'r', 'utf-8') as fp:
        eval_splits = json.load(fp)
    split_paper_ids = eval_splits[split]
    print(f'EVAL SPLIT: {split}; Number of queries: {len(split_paper_ids)}')
    with codecs.open(os.path.join(data_path, f'{dataset}-queries-release.csv')) as csvfile:
        reader = csv.DictReader(csvfile)
        query_metadata = dict([(row['paper_id'], row) for row in reader])
    
    perq_out_fname = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-{split}-perq.txt')
    qpid2rankedcand_relevances = read_unf_relevances(data_path=data_path, run_path=run_path,
                                                     dataset=dataset, method=method)
    # Go over test papers and compute metrics.
    all_metrics = {}
    num_cands = 0.0
    num_queries = 0.0
    perq_file = codecs.open(perq_out_fname, 'w', 'utf-8')
    perq_csv = csv.DictWriter(perq_file, extrasaction='ignore',
                              fieldnames=['paper_id', 'title',
                                          'recall@5', 'precision@5', 'f1@5',
                                          'recall@10', 'precision@10', 'f1@10',
                                          'recall@20', 'precision@20', 'f1@20',
                                          'r_precision', 'av_precision', 'reciprocal_rank', 'ndcg'])
    perq_csv.writeheader()
    print('Precision and recall at rank: {:}'.format(ATKS))
    for qpid in split_paper_ids:
        qranked_judgements = qpid2rankedcand_relevances[qpid]
        threshold = 1 if dataset in {'treccovid', 'scidcite', 'scidcocite', 'scidcoread', 'scidcoview'} else 2
        all_metrics[qpid] = compute_metrics(qranked_judgements, pr_atks=ATKS,
                                            threshold_grade=threshold)
        num_cands += len(qranked_judgements)
        num_queries += 1
        metrics = all_metrics[qpid]
        metrics['paper_id'] = qpid
        metrics['title'] = query_metadata[qpid]['title']
        perq_csv.writerow(metrics)
    aggmetrics = aggregate_metrics(query_metrics=all_metrics)
    print('Wrote: {:s}'.format(perq_file.name))
    perq_file.close()
    print('Total queries: {:d}; Total candidates: {:d}'.format(int(num_queries), int(num_cands)))
    # Precision and recall dont make too much sense.
    print('R-Precision; Precision@5; Precision@10; Precision@20; Recall@20; MAPrecision; NDCG; NDCG@20; NDCG%20')
    print('{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\n'.
          format(aggmetrics['r_precision'],
                 aggmetrics['precision@5'], aggmetrics['precision@10'], aggmetrics['precision@20'],
                 aggmetrics['recall@20'], aggmetrics['mean_av_precision'], aggmetrics['ndcg'],
                 aggmetrics['ndcg@20'], aggmetrics['ndcg%20']))
    # print('NDCG; NDCG%5; NDCG%10; NDCG%15; NDCG%20; NDCG%25')
    # print('{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\n'.
    #       format(aggmetrics['ndcg'], aggmetrics['ndcg%5'], aggmetrics['ndcg%10'], aggmetrics['ndcg%15'],
    #              aggmetrics['ndcg%20'], aggmetrics['ndcg%25']))
    if comet_exp_key:
        # Log metrics to comet.ml.
        run_name = os.path.basename(run_path)
        cml_experiment = cml.ExistingExperiment(previous_experiment=comet_exp_key, display_summary_level=0)
        cml_experiment.set_name(run_name)
        display_metrics = {'r_precision': aggmetrics['r_precision'],
                           'precision@5': aggmetrics['precision@5'],
                           'precision@10': aggmetrics['precision@10'],
                           'precision@20': aggmetrics['precision@20'],
                           'recall@20': aggmetrics['recall@20'],
                           'ndcg': aggmetrics['ndcg'],
                           'mean_av_precision': aggmetrics['mean_av_precision']}
        cml_experiment.log_metrics(display_metrics, prefix=f'{dataset}-{split}')
        cml_experiment.log_table(perq_out_fname, prefix=f'{dataset}-{split}')
    

def eval_significance_faceted(perq_path1, method1, perq_path2, method2, num_baseline_comparisions=2):
    """
    Given the per query results for different methods on disk, compute
    the statistical significance of the diff between the two results for
    a hardcoded set of metrics and for all of the facets.
    :param num_baseline_comparisions: int; this indicates the number of comparisions being made
        to apply a Bonferroni correction.
    :return:
    """
    significance_level_5 = 0.05/num_baseline_comparisions
    significance_level_10 = 0.10/num_baseline_comparisions
    metrics = ['r_precision', 'recall@20', 'av_precision', 'ndcg']
    for facet in ['background', 'method', 'result', 'all']:
        m1pid2metrics = {}
        try:
            fp = codecs.open(os.path.join(perq_path1, f'test-pid2pool-csfcube-{method1}-{facet}-perq.csv'), 'r', 'utf-8')
        except FileNotFoundError:
            fp = codecs.open(os.path.join(perq_path1, f'test-pid2pool-csfcube-{method1}-{facet}-perq.txt'), 'r', 'utf-8')
        reader = csv.DictReader(fp)
        for res_row in reader:
            m1pid2metrics[res_row['paper_id']] = res_row
        m2pid2metrics = {}
        try:
            fp = codecs.open(os.path.join(perq_path2, f'test-pid2pool-csfcube-{method2}-{facet}-perq.csv'), 'r', 'utf-8')
        except FileNotFoundError:
            fp = codecs.open(os.path.join(perq_path2, f'test-pid2pool-csfcube-{method2}-{facet}-perq.txt'), 'r', 'utf-8')
        reader = csv.DictReader(fp)
        for res_row in reader:
            m2pid2metrics[res_row['paper_id']] = res_row
        metric2pval = {}
        for metric in metrics:
            m1, m2 = [], []
            for qpid in m1pid2metrics.keys():
                m1.append(float(m1pid2metrics[qpid][metric]))
                m2.append(float(m2pid2metrics[qpid][metric]))
            tval, pval = scipystats.ttest_ind(m1, m2, equal_var=False, nan_policy='propagate')
            metric2pval[metric] = pval
        print('Method 1: {:s}; Method 2: {:s}; facet: {:s}'.format(method1, method2, facet))
        print('R-Precision; Recall@20; MAP; NDCG; P-Values:')
        print('0.05; Bonferroni corrected significance: {:.4f}'.format(significance_level_5))
        print('{:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}'.
              format(True if metric2pval['r_precision'] < significance_level_5 else False, metric2pval['r_precision'],
                     True if metric2pval['recall@20'] < significance_level_5 else False, metric2pval['recall@20'],
                     True if metric2pval['av_precision'] < significance_level_5 else False, metric2pval['av_precision'],
                     True if metric2pval['ndcg'] < significance_level_5 else False, metric2pval['ndcg']))
        print('0.10; Bonferroni corrected significance: {:.4f}'.format(significance_level_10))
        print('{:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}\n'.
              format(True if metric2pval['r_precision'] < significance_level_10 else False, metric2pval['r_precision'],
                     True if metric2pval['recall@20'] < significance_level_10 else False, metric2pval['recall@20'],
                     True if metric2pval['av_precision'] < significance_level_10 else False, metric2pval['av_precision'],
                     True if metric2pval['ndcg'] < significance_level_10 else False, metric2pval['ndcg']))


def eval_significance_unfaceted(perq_path1, method1, perq_path2, method2, dataset, num_baseline_comparisions=2):
    """
    Given the per query results for different methods on disk, compute
    the statistical significance of the diff between the two results for
    a hardcoded set of metrics and for all of the facets.
    :param num_baseline_comparisions: int; this indicates the number of comparisions being made
        to apply a Bonferroni correction.
    :return:
    """
    significance_level_5 = 0.05/num_baseline_comparisions
    significance_level_10 = 0.10/num_baseline_comparisions
    metrics = ['r_precision', 'recall@20', 'av_precision', 'ndcg']
    m1pid2metrics = {}
    try:
        fp = codecs.open(os.path.join(perq_path1, f'test-pid2pool-{dataset}-{method1}-test-perq.csv'), 'r', 'utf-8')
    except FileNotFoundError:
        fp = codecs.open(os.path.join(perq_path1, f'test-pid2pool-{dataset}-{method1}-test-perq.txt'), 'r', 'utf-8')
    reader = csv.DictReader(fp)
    for res_row in reader:
        m1pid2metrics[res_row['paper_id']] = res_row
    m2pid2metrics = {}
    try:
        fp = codecs.open(os.path.join(perq_path2, f'test-pid2pool-{dataset}-{method2}-test-perq.csv'), 'r', 'utf-8')
    except FileNotFoundError:
        fp = codecs.open(os.path.join(perq_path2, f'test-pid2pool-{dataset}-{method2}-test-perq.txt'), 'r', 'utf-8')
    reader = csv.DictReader(fp)
    for res_row in reader:
        m2pid2metrics[res_row['paper_id']] = res_row
    metric2pval = {}
    for metric in metrics:
        m1, m2 = [], []
        for qpid in m1pid2metrics.keys():
            m1.append(float(m1pid2metrics[qpid][metric]))
            m2.append(float(m2pid2metrics[qpid][metric]))
        tval, pval = scipystats.ttest_ind(m1, m2, equal_var=False, nan_policy='propagate')
        metric2pval[metric] = pval
    print('Method 1: {:s}; Method 2: {:s}'.format(method1, method2))
    print('R-Precision; Recall@20; MAP; NDCG; P-Values:')
    print('0.05; Bonferroni corrected significance: {:.4f}'.format(significance_level_5))
    print('{:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}'.
          format(True if metric2pval['r_precision'] < significance_level_5 else False, metric2pval['r_precision'],
                 True if metric2pval['recall@20'] < significance_level_5 else False, metric2pval['recall@20'],
                 True if metric2pval['av_precision'] < significance_level_5 else False, metric2pval['av_precision'],
                 True if metric2pval['ndcg'] < significance_level_5 else False, metric2pval['ndcg']))
    print('0.10; Bonferroni corrected significance: {:.4f}'.format(significance_level_10))
    print('{:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}\n'.
          format(True if metric2pval['r_precision'] < significance_level_10 else False, metric2pval['r_precision'],
                 True if metric2pval['recall@20'] < significance_level_10 else False, metric2pval['recall@20'],
                 True if metric2pval['av_precision'] < significance_level_10 else False, metric2pval['av_precision'],
                 True if metric2pval['ndcg'] < significance_level_10 else False, metric2pval['ndcg']))

    
def main():
    """
    Parse command line arguments and call all the above routines.
    :return:
    """
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='subcommand',
                                       help='The action to perform.')
    # Evaluate the re-ranked pools.
    evaluate_pool_ranks = subparsers.add_parser('eval_pool_ranking')
    evaluate_pool_ranks.add_argument('--data_path', required=True,
                                     help='Input path where file with ranked candidates'
                                          'and gold data are if its a baseline method.')
    evaluate_pool_ranks.add_argument('--run_path',
                                     help='Input path where file with ranked candidates. Model run dir.')
    evaluate_pool_ranks.add_argument('--run_name', default=None,
                                     help='Base name of directory with specific model runs embeddings.s')
    evaluate_pool_ranks.add_argument('--experiment', required=True,
                                     choices=['specter', 'sbtinybertsota', 'sbrobertanli',
                                              'sbmpnet1B', 'cosentbert', 'ictsentbert', 'cospecter',
                                              'labspecter', 'miswordbienc', 'supsimcse', 'unsupsimcse',
                                              'miswordpolyenc', 'sbalisentbienc'],
                                     help='The experiment to evaluate.')
    evaluate_pool_ranks.add_argument('--dataset', required=True,
                                     choices=['csfcube', 'relish', 'treccovid',
                                              'scidcite', 'scidcocite', 'scidcoread', 'scidcoview'],
                                     help='The dataset to evaluate for.')
    evaluate_pool_ranks.add_argument('--facet',
                                     choices=['background', 'method', 'result', 'all'],
                                     help='Facet of abstract to read from.')
    evaluate_pool_ranks.add_argument('--comet_exp_key', default=None,
                                     help='Hash for comet experiment run. Goto copy this correctly.')
    cl_args = parser.parse_args()

    if cl_args.subcommand == 'eval_pool_ranking':
        try:
            facet = cl_args.facet
        except AttributeError:
            facet = None
        if cl_args.experiment in {'specter', 'sbtinybertsota', 'sbrobertanli', 'ictsentbert',
                                  'supsimcse', 'unsupsimcse', 'sbmpnet1B'}:
            run_path = os.path.join(cl_args.data_path, cl_args.experiment)
        else:
            run_path = cl_args.run_path
        if cl_args.dataset in {'csfcube'}:
            graded_eval_pool_rerank_faceted(data_path=cl_args.data_path, method=cl_args.experiment,
                                            facet=facet, dataset=cl_args.dataset, run_path=run_path,
                                            split_str='dev', comet_exp_key=cl_args.comet_exp_key)
            print('\n')
            graded_eval_pool_rerank_faceted(data_path=cl_args.data_path, method=cl_args.experiment,
                                            facet=facet, dataset=cl_args.dataset, run_path=run_path,
                                            split_str='test', comet_exp_key=cl_args.comet_exp_key)
        elif cl_args.dataset in {'relish', 'treccovid', 'scidcite', 'scidcocite', 'scidcoread', 'scidcoview'}:
            graded_eval_pool_rerank_unf(data_path=cl_args.data_path, method=cl_args.experiment,
                                        dataset=cl_args.dataset, run_path=run_path, split='dev',
                                        comet_exp_key=cl_args.comet_exp_key)
            print('\n\n')
            graded_eval_pool_rerank_unf(data_path=cl_args.data_path, method=cl_args.experiment,
                                        dataset=cl_args.dataset, run_path=run_path, split='test',
                                        comet_exp_key=cl_args.comet_exp_key)
    elif cl_args.subcommand == 'result_signf':
        if cl_args.dataset == 'csfcube':
            eval_significance_faceted(perq_path1=cl_args.run_path_1, method1=cl_args.method_1,
                                      perq_path2=cl_args.run_path_2, method2=cl_args.method_2)
        else:
            eval_significance_unfaceted(perq_path1=cl_args.run_path_1, method1=cl_args.method_1,
                                        perq_path2=cl_args.run_path_2, method2=cl_args.method_2,
                                        dataset=cl_args.dataset)
    else:
        sys.stderr.write("Unknown action.")


if __name__ == '__main__':
    main()
